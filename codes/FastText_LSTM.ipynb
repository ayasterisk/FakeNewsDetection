{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyi55ggKu21K"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/sample_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1fli_hyDy7Io0coUNdk1P-DUPWtpfBwsX"
      ],
      "metadata": {
        "id": "ZDizvwLNu8K7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d47919c1-5ddb-4e67-bd8a-16862905d829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fli_hyDy7Io0coUNdk1P-DUPWtpfBwsX\n",
            "To: /content/news-NLP.csv\n",
            "100% 30.7M/30.7M [00:00<00:00, 87.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.models import FastText\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "xZvOIWKxvCUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BzvQ5-I3tU1",
        "outputId": "0029ce03-9674-4d03-bb70-73a5b4b9a344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('news-NLP.csv')\n",
        "df = df.drop(df.columns[0], axis=1)"
      ],
      "metadata": {
        "id": "K8LnKK1Y5EA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = df['label'].apply(lambda x: 1 if x == \"FAKE\" else 0)\n",
        "df['content'] = df['title'] + ' ' + df['text']"
      ],
      "metadata": {
        "id": "IoGz6Z205EqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "rSPjxmDI5Kap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return words"
      ],
      "metadata": {
        "id": "YpbJFZii5NDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['processed_content'] = df['content'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "G7wK2Nor5xoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model = FastText(sentences=df['processed_content'], vector_size=100, window=5, min_count=5, workers=4, sg=0, epochs=10)"
      ],
      "metadata": {
        "id": "1aH1Kp_UORfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((len(fasttext_model.wv.key_to_index) + 1, 100))  # +1 for padding\n",
        "word_index = {word: idx + 1 for idx, word in enumerate(fasttext_model.wv.key_to_index)}\n",
        "for word, idx in word_index.items():\n",
        "    embedding_matrix[idx] = fasttext_model.wv[word]"
      ],
      "metadata": {
        "id": "m63cc1OnPRV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sequence(text, word_index):\n",
        "    return [word_index[word] for word in text if word in word_index]\n",
        "\n",
        "df['sequence'] = df['processed_content'].apply(lambda x: text_to_sequence(x, word_index))"
      ],
      "metadata": {
        "id": "obJ3Uz0lPgpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 200\n",
        "X = pad_sequences(df['sequence'], maxlen=max_seq_len, padding='post')\n",
        "y = df['label'].values"
      ],
      "metadata": {
        "id": "sHG4mRJWPlZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "KuhHkOa8PoB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=len(embedding_matrix),\n",
        "        output_dim=100,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_seq_len,\n",
        "        trainable=False  # Freeze embedding layer\n",
        "    ),\n",
        "    LSTM(128, return_sequences=False),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSVJDLgrrIaT",
        "outputId": "57dfdb47-a34e-4fbb-f098-30e3bf6bf990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.6910 - loss: 0.5875 - val_accuracy: 0.7978 - val_loss: 0.4211\n",
            "Epoch 2/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8517 - loss: 0.3838 - val_accuracy: 0.8393 - val_loss: 0.3844\n",
            "Epoch 3/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8709 - loss: 0.3242 - val_accuracy: 0.8432 - val_loss: 0.3725\n",
            "Epoch 4/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8990 - loss: 0.2822 - val_accuracy: 0.8560 - val_loss: 0.3590\n",
            "Epoch 5/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9214 - loss: 0.2200 - val_accuracy: 0.8531 - val_loss: 0.3785\n",
            "Epoch 6/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9329 - loss: 0.1978 - val_accuracy: 0.7396 - val_loss: 0.6280\n",
            "Epoch 7/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8287 - loss: 0.3469 - val_accuracy: 0.8432 - val_loss: 0.4250\n",
            "Epoch 8/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9492 - loss: 0.1613 - val_accuracy: 0.8501 - val_loss: 0.4080\n",
            "Epoch 9/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9188 - loss: 0.2043 - val_accuracy: 0.8501 - val_loss: 0.4481\n",
            "Epoch 10/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9454 - loss: 0.1590 - val_accuracy: 0.8402 - val_loss: 0.4371\n",
            "Epoch 11/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9568 - loss: 0.1319 - val_accuracy: 0.8274 - val_loss: 0.4975\n",
            "Epoch 12/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9582 - loss: 0.1274 - val_accuracy: 0.8422 - val_loss: 0.5164\n",
            "Epoch 13/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9630 - loss: 0.1170 - val_accuracy: 0.8422 - val_loss: 0.5376\n",
            "Epoch 14/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9318 - loss: 0.1634 - val_accuracy: 0.8156 - val_loss: 0.5166\n",
            "Epoch 15/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9178 - loss: 0.1753 - val_accuracy: 0.8235 - val_loss: 0.5461\n",
            "Epoch 16/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9350 - loss: 0.1518 - val_accuracy: 0.8037 - val_loss: 0.6182\n",
            "Epoch 17/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9448 - loss: 0.1594 - val_accuracy: 0.8383 - val_loss: 0.4933\n",
            "Epoch 18/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9657 - loss: 0.1080 - val_accuracy: 0.8235 - val_loss: 0.5136\n",
            "Epoch 19/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9518 - loss: 0.1276 - val_accuracy: 0.8491 - val_loss: 0.5016\n",
            "Epoch 20/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9580 - loss: 0.1191 - val_accuracy: 0.8491 - val_loss: 0.5227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (model.predict(X_test) > 0.5).astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF3yK0ADP7Z8",
        "outputId": "ec8a0c99-1688-421e-fb87-517e5cbe81ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbNwdOUGuBO-",
        "outputId": "03e3db5a-21c2-4071-bd53-02857c88d099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.846093133385951\n",
            "Precision: 0.8431061806656102\n",
            "Recall: 0.8471337579617835\n",
            "F1 Score: 0.8451151707704527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model_fasttext_lstm.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiFpRhzyQOWC",
        "outputId": "5656b6b7-ab0a-464a-9215-a4ddf18ad929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.models import FastText\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "fasttext_model = FastText.load(\"fasttext_model_lstm.bin\")\n",
        "model = load_model(\"model_fasttext_lstm.h5\")\n",
        "\n",
        "# Word index from the training process (make sure this matches the training data)\n",
        "word_index = {word: idx + 1 for idx, word in enumerate(fasttext_model.wv.key_to_index)}\n",
        "\n",
        "# Preprocessing function (same as in training)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "def text_to_sequence(text, word_index):\n",
        "    return [word_index[word] for word in text if word in word_index]\n",
        "\n",
        "def predict_fake_news(news_text):\n",
        "    processed_text = preprocess_text(news_text)\n",
        "    sequence = text_to_sequence(processed_text, word_index)\n",
        "    max_seq_len = 200  # Same max sequence length as in training\n",
        "    padded_sequence = pad_sequences([sequence], maxlen=max_seq_len, padding='post')\n",
        "    prediction = model.predict(padded_sequence)[0][0]\n",
        "    print(prediction)\n",
        "    if prediction > 0.5:\n",
        "        return \"FAKE\"\n",
        "    else:\n",
        "        return \"REAL\"\n",
        "\n",
        "\n",
        "news_text = \"This is a sample sentence to check if it is real or fake.\"\n",
        "prediction = predict_fake_news(news_text)\n",
        "print(f\"The news is predicted to be: {prediction}\")"
      ],
      "metadata": {
        "id": "j4nHNiQdQ1T6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c36fcc-54c6-46d8-b8b2-3f688ca90fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
            "0.7883114\n",
            "The news is predicted to be: FAKE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jF5bdXXpYlOp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}